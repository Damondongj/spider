{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import requests\n",
    "from bs4 import Tag\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    " \n",
    "def getHtml(url):\n",
    "    page = requests.get(url)\n",
    "    html =page.text\n",
    "    return html\n",
    " \n",
    "def getText(html):\n",
    "    get_text = Tag.get_text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    author_info = soup.find_all('div', class_='atl-info')\n",
    "    listauthor  = [x.get_text() for x in author_info]\n",
    "        \n",
    "    list_info = soup.find_all('div', class_='bbs-content')\n",
    "    listtext  = [x.get_text() for x in list_info]\n",
    " \n",
    "    global i\n",
    "    if i > 1:\n",
    "        listtext = [\"\"] + listtext\n",
    "    \n",
    "    for x in range(len(listauthor)):\n",
    "        if \"楼主\" in listauthor[x]:\n",
    "            print (listtext[x].strip())\n",
    "            \n",
    "if __name__=='__main__':\n",
    "    for i in range(1,6):\n",
    "        url  = (\"http://bbs.tianya.cn/post-feeling-4286798-%s.shtml\" % str(i))\n",
    "        html = getHtml(url)\n",
    "        getText(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting the 1 page\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import xlsxwriter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_html(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers = headers)\n",
    "        response.encoding = 'gbk'  # 编码方式问题\n",
    "        html = response.text\n",
    "        return html\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def getMovie(html):\n",
    "    allmovies = []\n",
    "    movieurl = []\n",
    "    ftpurl = []\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    url_info = soup.find_all(\"a\", class_=\"ulink\")\n",
    "    for info in url_info:\n",
    "        pattern = \"《.*?》\"\n",
    "        movies = re.findall(pattern, info.get_text())\n",
    "        for movie in movies:\n",
    "            allmovies.append(movie)\n",
    "        reurl = \"https://www.dydytt.net\" + info.get(\"href\")\n",
    "        movieurl.append(reurl)\n",
    "    for url in movieurl:\n",
    "        try:\n",
    "            html = getHtml(url)\n",
    "            reg = r'href=\"(ftp:.+?)\">'\n",
    "            imgre = re.compile(reg)\n",
    "            imaglist, *_ = re.findall(imgre, html)\n",
    "            print(imaglist)\n",
    "            ftpurl.append(imaglist)\n",
    "        except:\n",
    "            print(\"\")\n",
    "            ftpurl.append(\"\")\n",
    "    return allmovies, ftpurl\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    workbook = xlsxwriter.Workbook(\"movie_source.xlsx\")\n",
    "    worksheet = workbook.add_worksheet()\n",
    "    row = 0\n",
    "    for i in range(1, 166):\n",
    "        print(\"Visiting the \" + str(i) + \" page\")\n",
    "        url = \"https://www.dydytt.net/html/tv/oumeitv/list_9_\" + str(i) + \".html\"\n",
    "        html = get_html(url)\n",
    "        if not html:\n",
    "            print(\"Access Exception\")\n",
    "            continue\n",
    "        movie, ftp = getMovie(html)\n",
    "        for item in zip(movie, ftp):\n",
    "            worksheet.write(row, 0. item[0])\n",
    "            worksheet.write(row, 4, item[1])\n",
    "            row = row + 1\n",
    "    workbook.close()\n",
    "    print(\"Download finished\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajax demo\n",
    "# The key point of ajax is find the request url and the changing parameters.\n",
    "# leanred from https://cuiqingcai.com/202253.html\n",
    "import requests\n",
    "import logging\n",
    "import pymongo\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format=\"%(asctime)s - %(levelname)s: %(message)s\")\n",
    "\n",
    "INDEX_URL = \"https://spa1.scrape.center/api/movie/?limit={limit}&offset={offset}\"\n",
    "DETAIL_URL = \"https://spa1.scrape.center/api/movie/{id}\"\n",
    "\n",
    "MONGO_CONNECTION_STRING = \"mongodb://localhost:27017\"\n",
    "MONGO_DB_NAME = \"movies\"  # mongodb 数据库名称\n",
    "MONGO_COLLECTION_NAME = \"movies\" # mongodb 的集合名称\n",
    "\n",
    "LIMIT = 10\n",
    "TOTAL_PAGE = 10\n",
    "\n",
    "def scrape_api(url):\n",
    "    logging.info(\"scraping %s...\", url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        logging.error(\"get invalid status code %s while scraping %s\", response.status_code, url)\n",
    "    except requests.RequestException:\n",
    "        logging.error(\"error occurred while scraping %s\", url, exc_info=True)\n",
    "\n",
    "def scrape_index(page):\n",
    "    url = INDEX_URL.format(limit=LIMIT, offset=LIMIT * (page -1))\n",
    "    return scrape_api(url)\n",
    "\n",
    "def scrape_detail(id):\n",
    "    url = DETAIL_URL.format(id=id)\n",
    "    return scrape_api(url)\n",
    "\n",
    "client = pymongo.MongoClient(MONGO_CONNECTION_STRING)\n",
    "db = client[\"movies\"]\n",
    "collection = db[\"movies\"]\n",
    "\n",
    "def save_data(data):\n",
    "    collection.update_one({\n",
    "        \"name\": data.get(\"name\")\n",
    "    }, {\n",
    "        \"$set\": data\n",
    "    }, upsert=True)\n",
    "\n",
    "def main():\n",
    "    for page in range(1, TOTAL_PAGE + 1):\n",
    "        index_data = scrape_index(page)\n",
    "        for item in index_data.get(\"results\"):\n",
    "            id = item.get(\"id\")\n",
    "            detail_data = scrape_detail(id)\n",
    "            logging.info(\"detail data %s\", detail_data)\n",
    "            save_data(detail_data)\n",
    "            logging.info(\"data saved successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import logging \n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s : %(message)s\")\n",
    "\n",
    "TOTAL_NUMBER = 100\n",
    "URL = \"https://httpbin.org/delay/5\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(1, TOTAL_NUMBER + 1):\n",
    "    logging.info(\"scraping %s\", URL)\n",
    "    response = requests.get(URL)\n",
    "end_time = time.time()\n",
    "\n",
    "logging.info(\"total cost time %s seconds\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def execute(x):\n",
    "    print(\"Number: \", x)\n",
    "\n",
    "coroutine = execute(1)\n",
    "print(\"Coroutine: \", coroutine)\n",
    "print(\"After calling execute\")\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(coroutine)\n",
    "print(\"After calling loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import requests\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "async def get(url):\n",
    "    session = aiohttp.ClientSession()\n",
    "    response = await session.get(url)\n",
    "    await response.text()\n",
    "    await session.close()\n",
    "    return response\n",
    "\n",
    "async def request():\n",
    "    url = \"https://httpbin.org/delay/5\"\n",
    "    print(\"waiting for\", url)\n",
    "    response = await get(url)\n",
    "    print(f\"Get response from {url}, response: {response}\")\n",
    "\n",
    "tasks = [asyncio.ensure_future(request()) for _ in range(10)]\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(asyncio.wait(tasks))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Cost time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "async def get(url):\n",
    "    session = aiohttp.ClientSession()\n",
    "    response = await session.get(url)\n",
    "    await response.text()\n",
    "    await session.close()\n",
    "    return response\n",
    "\n",
    "async def request():\n",
    "    url = 'https://httpbin.org/delay/5'\n",
    "    print('Waiting for', url)\n",
    "    response = await get(url)\n",
    "    print('Get response from', url, 'response', response)\n",
    "\n",
    "tasks = [asyncio.ensure_future(request()) for _ in range(10)]\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(asyncio.wait(tasks))\n",
    "\n",
    "end = time.time()\n",
    "print('Cost time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "\n",
    "def test(number):\n",
    "    start = time.time()\n",
    "\n",
    "    async def get(url):\n",
    "        session = aiohttp.ClientSession()\n",
    "        response = await session.get(url)\n",
    "        await response.text()\n",
    "        await session.close()\n",
    "        return response\n",
    "\n",
    "    async def request():\n",
    "        url = 'https://www.baidu.com/'\n",
    "        await get(url)\n",
    "\n",
    "    tasks = [asyncio.ensure_future(request()) for _ in range(number)]\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(asyncio.wait(tasks))\n",
    "\n",
    "    end = time.time()\n",
    "    print('Number:', number, 'Cost time:', end - start)\n",
    "\n",
    "for number in [1, 3, 5, 10, 15, 30, 50, 75, 100, 200, 500]:\n",
    "    test(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9cfffd19ca5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENTER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mwait\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mwait\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpresence_of_element_located\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"content-left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\star\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py\u001b[0m in \u001b[0;36muntil\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0muntil_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.webdriver.support.wait import WebDriverWait \n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "try:\n",
    "    browser.get(\"https://www.baidu.com\")\n",
    "    input = browser.find_element_by_id(\"kw\")\n",
    "    input.send_keys(\"python\")\n",
    "    input.send_keys(Keys.ENTER)\n",
    "    # 显示等待\n",
    "    wait = WebDriverWait(browser, 10)\n",
    "    wait.until(EC.presence_of_element_located((By.ID, \"content-left\")))\n",
    "    print(browser.current_url)\n",
    "    print(browser.get_cookies())\n",
    "    # page_source 网页源代码\n",
    "    print(browser.page_source)\n",
    "finally:\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ChromeOptions\n",
    "\n",
    "option = ChromeOptions()\n",
    "option.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "option.add_experimental_option('useAutomationExtension', False)\n",
    "browser = webdriver.Chrome(options=option)\n",
    "browser.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "    'source': 'Object.defineProperty(navigator, \"webdriver\", {get: () => undefined})'\n",
    "})\n",
    "browser.get('https://antispider1.scrape.center/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ChromeOptions\n",
    "\n",
    "# option = ChromeOptions()\n",
    "# option.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "# option.add_experimental_option('useAutomationExtension', False)\n",
    "# browser = webdriver.Chrome(options=option)\n",
    "browser = webdriver.Chrome()\n",
    "# browser.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "#     'source': 'Object.defineProperty(navigator, \"webdriver\", {get: () => undefined})'\n",
    "# })\n",
    "browser.get('https://antispider1.scrape.center/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookies:  <RequestsCookieJar[<Cookie sessionid=7c310hdo1i80258o84e7t30p50qjohos for login2.scrape.center/>]>\n",
      "Response Status 200\n",
      "Response URL https://login2.scrape.center/page/1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE_URL = 'https://login2.scrape.center/'\n",
    "LOGIN_URL = urljoin(BASE_URL, '/login')\n",
    "INDEX_URL = urljoin(BASE_URL, '/page/1')\n",
    "USERNAME = 'admin'\n",
    "PASSWORD = 'admin'\n",
    "\n",
    "# requests直接调用post、get等方法，每次请求都是一个独立的请求，这都相当于是\n",
    "# 新开一个浏览器打开这些链接，所以这两次请求对应的session并不是同一个，这里我们模拟第一个session登录，并不能影响第二个session的状态\n",
    "# 因此模拟登录也就无效了。\n",
    "response_login = requests.post(LOGIN_URL, data={\n",
    "    'username': USERNAME,\n",
    "    'password': PASSWORD\n",
    "}, allow_redirects=False) # requests可以自动处理重定向，我们在模拟登录的时候要加上allow_redirects参数将其设置为False\n",
    "\n",
    "\n",
    "cookies = response_login.cookies\n",
    "print(\"Cookies: \", cookies)\n",
    "\n",
    "response_index = requests.get(INDEX_URL, cookies=cookies)\n",
    "print('Response Status', response_index.status_code)\n",
    "print('Response URL', response_index.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookies:  <RequestsCookieJar[<Cookie sessionid=a7jcxnw3y3exsw038339txjaj0vr9ree for login2.scrape.center/>]>\n",
      "Response status:  200\n",
      "Response URL:  https://login2.scrape.center/page/1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE_URL = 'https://login2.scrape.center/'\n",
    "LOGIN_URL = urljoin(BASE_URL, '/login')\n",
    "INDEX_URL = urljoin(BASE_URL, '/page/1')\n",
    "USERNAME = 'admin'\n",
    "PASSWORD = 'admin'\n",
    "\n",
    "# 无须关心Cookie的处理和传递问题，声明一个Session对象\n",
    "# 然后每次调用请求的时候都直接使用Session对象的post或get方法\n",
    "session = requests.Session()\n",
    "\n",
    "response_login = session.post(LOGIN_URL, data={\n",
    "    \"username\": USERNAME,\n",
    "    \"password\": PASSWORD\n",
    "})\n",
    "\n",
    "cookies = session.cookies\n",
    "print(\"Cookies: \", cookies)\n",
    "\n",
    "response_index = session.get(INDEX_URL)\n",
    "print(\"Response status: \", response_index.status_code)\n",
    "print(\"Response URL: \", response_index.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookies:  [{'domain': 'login2.scrape.center', 'expiry': 1677740342, 'httpOnly': True, 'name': 'sessionid', 'path': '/', 'sameSite': 'Lax', 'secure': False, 'value': 'rqy0w4gz0ujmpibrw5d70ehhmrnu606b'}]\n",
      "Response Status:  200\n",
      "Response URL https://login2.scrape.center/page/1\n"
     ]
    }
   ],
   "source": [
    "# 使用selenium模拟登录，获取cookies\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import time\n",
    "\n",
    "BASE_URL = 'https://login2.scrape.center/'\n",
    "LOGIN_URL = urljoin(BASE_URL, '/login')\n",
    "INDEX_URL = urljoin(BASE_URL, '/page/1')\n",
    "USERNAME = 'admin'\n",
    "PASSWORD = 'admin'\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(BASE_URL)\n",
    "browser.find_element_by_css_selector('input[name=\"username\"]').send_keys(USERNAME)\n",
    "browser.find_element_by_css_selector('input[name=\"password\"]').send_keys(PASSWORD)\n",
    "browser.find_element_by_css_selector('input[type=\"submit\"]').click()\n",
    "time.sleep(10)\n",
    "\n",
    "cookies = browser.get_cookies()\n",
    "print(\"Cookies: \", cookies)\n",
    "browser.close()\n",
    "\n",
    "session = requests.Session()\n",
    "for cookie in cookies:\n",
    "    session.cookies.set(cookie[\"name\"], cookie[\"value\"])\n",
    "    \n",
    "response_index = session.get(INDEX_URL)\n",
    "print(\"Response Status: \", response_index.status_code)\n",
    "print(\"Response URL\", response_index.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 requests\n",
      "Finished 2 requests\n",
      "Finished 3 requests\n",
      "Finished 4 requests\n",
      "Finished 5 requests\n",
      "Finished 6 requests\n",
      "Finished 7 requests\n",
      "Finished 8 requests\n",
      "Finished 9 requests\n",
      "Finished 10 requests\n",
      "Cost time:  14.763051509857178\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "session = requests.Session()\n",
    "for _ in range(10):\n",
    "    session.get(\"http://httpbin.org/delay/1\")\n",
    "    print(f\"Finished { _ + 1} requests\")\n",
    "end = time.time()\n",
    "print(\"Cost time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\star\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pymongo\\pyopenssl_context.py:26: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography.x509 import load_der_x509_certificate as _load_der_x509_certificate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 requests\n",
      "Finished 2 requests\n",
      "Finished 3 requests\n",
      "Finished 4 requests\n",
      "Finished 5 requests\n",
      "Finished 6 requests\n",
      "Finished 7 requests\n",
      "Finished 8 requests\n",
      "Finished 9 requests\n",
      "Finished 10 requests\n",
      "Cost time:  1.8296372890472412\n"
     ]
    }
   ],
   "source": [
    "import requests_cache\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "session = requests_cache.CachedSession(\"demo_cache\")\n",
    "\n",
    "for _ in range(10):\n",
    "    session.get(\"http://httpbin.org/delay/1\")\n",
    "    print(f\"Finished { _ + 1} requests\")\n",
    "end = time.time()\n",
    "print(\"Cost time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 requests\n",
      "Finished 2 requests\n",
      "Finished 3 requests\n",
      "Finished 4 requests\n",
      "Finished 5 requests\n",
      "Finished 6 requests\n",
      "Finished 7 requests\n",
      "Finished 8 requests\n",
      "Finished 9 requests\n",
      "Finished 10 requests\n",
      "Cost time:  0.027999401092529297\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "import time\n",
    "\n",
    "# use_temp 缓存文件夹便会使用系统的临时目录，而不会在代码区创建缓存文件夹\n",
    "requests_cache.install_cache(\"demo_cache\", backend=\"filesystem\", use_temp=True)\n",
    "\n",
    "# use_cache_dir参数，缓存文件夹便会使用系统的专用缓存文件夹，而不会在代码区创建缓存文件夹。\n",
    "# requests_cache.install_cache(\"demo_cache\", backend=\"filesystem\", use_cache_dir=True)\n",
    "\n",
    "# 使用 redis \n",
    "# backend = requests_cache.RedisCache(host=\"localhost\", port=6379)\n",
    "# requests_cache.install_cache(\"demo_cache\", backend=backend)\n",
    "\n",
    "start = time.time()\n",
    "session = requests.Session()\n",
    "for _ in range(10):\n",
    "    session.get(\"http://httpbin.org/delay/1\")\n",
    "    print(f\"Finished {_ + 1} requests\")\n",
    "end = time.time()\n",
    "print(\"Cost time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 requests\n",
      "Finished 2 requests\n",
      "Finished 3 requests\n",
      "Finished 4 requests\n",
      "Finished 5 requests\n",
      "Finished 6 requests\n",
      "Finished 7 requests\n",
      "Finished 8 requests\n",
      "Finished 9 requests\n",
      "Finished 10 requests\n",
      "Cost time for get 14.257228374481201\n",
      "Finished 1 requests\n",
      "Finished 2 requests\n",
      "Finished 3 requests\n",
      "Finished 4 requests\n",
      "Finished 5 requests\n",
      "Finished 6 requests\n",
      "Finished 7 requests\n",
      "Finished 8 requests\n",
      "Finished 9 requests\n",
      "Finished 10 requests\n",
      "Cost time for post 0.017030000686645508\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import requests_cache\n",
    "\n",
    "# allowable_codes(200, ) 只有响应为200的会缓存\n",
    "requests_cache.install_cache(\"demo_cache2\", allowable_methods=[\"POST\"])\n",
    "\n",
    "\n",
    "# 匹配 url，针对哪种 pattern 的url缓存多久\n",
    "# site_1.com 的内容会缓存 30s， site_2.com/static 的内容永远不会过期\n",
    "# urls_expire_after = {\"*.site_1.com\": 30, \"site_2.com/static\": -1}\n",
    "# request_cache.install_cache(\n",
    "#     \"demo_cache2\", urls_expire_after=urls_expire_after\n",
    "# )\n",
    "\n",
    "start = time.time()\n",
    "session = requests.Session()\n",
    "for i in range(10):\n",
    "    session.get(\"http://httpbin.org/delay/1\")\n",
    "    print(f\"Finished {i + 1} requests\")\n",
    "end = time.time()\n",
    "print(\"Cost time for get\", end - start)\n",
    "start = time.time()\n",
    "\n",
    "for i in range(10):\n",
    "    session.post(\"http://httpbin.org/delay/1\")\n",
    "    print(f\"Finished {i + 1} requests\")\n",
    "end = time.time()\n",
    "print(\"Cost time for post\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import tesserocr\n",
    "from selenium import webdriver\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from retrying import retry\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    image = image.convert('L')\n",
    "    array = np.array(image)\n",
    "    array = np.where(array > 50, 255, 0)\n",
    "    image = Image.fromarray(array.astype('uint8'))\n",
    "    return image\n",
    "\n",
    "\n",
    "@retry(stop_max_attempt_number=10, retry_on_result=lambda x: x is False)\n",
    "def login():\n",
    "    browser.get('https://captcha7.scrape.center/')\n",
    "    browser.find_element_by_css_selector('.username input[type=\"text\"]').send_keys('admin')\n",
    "    browser.find_element_by_css_selector('.password input[type=\"password\"]').send_keys('admin')\n",
    "    captcha = browser.find_element_by_css_selector('#captcha')\n",
    "    image = Image.open(BytesIO(captcha.screenshot_as_png))\n",
    "    image = preprocess(image)\n",
    "    captcha = tesserocr.image_to_text(image)\n",
    "    captcha = re.sub('[^A-Za-z0-9]', '', captcha)\n",
    "    browser.find_element_by_css_selector('.captcha input[type=\"text\"]').send_keys(captcha)\n",
    "    browser.find_element_by_css_selector('.login').click()\n",
    "    try:\n",
    "        WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, '//h2[contains(., \"登录成功\")]')))\n",
    "        time.sleep(10)\n",
    "        browser.close()\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    browser = webdriver.Chrome()\n",
    "    login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ChromeOptions\n",
    "\n",
    "option = ChromeOptions()\n",
    "option.add_argument('--headless')\n",
    "browser = webdriver.Chrome(options=option)\n",
    "browser.set_window_size(1366, 768)\n",
    "browser.get('https://www.baidu.com')\n",
    "browser.get_screenshot_as_file('preview.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(r\"C:\\Users\\star\\Desktop\\a.jpg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_, threshold = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "opening = cv2.morphologyEx(threshold, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "contours, _ = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "for cnt in contours:\n",
    "    area = cv2.contourArea(cnt)\n",
    "    if area > 100:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        img[y:y+h, x:x+w] = (255, 255, 255)\n",
    "        \n",
    "cv2.imwrite(r\"C:\\Users\\star\\Desktop\\b.jpg\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(r\"C:\\Users\\star\\Desktop\\a.jpg\")\n",
    "img = cv2.resize(img, None, fx=0.2, fy=0.2, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "img_blur = cv2.GaussianBlur(img, (25, 25), 0)\n",
    "\n",
    "img_pixelate = cv2.resize(img_blur, None, fx=0.05, fy=0.05, interpolation=cv2.INTER_NEAREST)\n",
    "img_pixelate = cv2.resize(img_pixelate, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "img_mosaic = cv2.resize(img_pixelate, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "img_restored = cv2.addWeighted(img, 0.8, img_mosaic, 0.2, 0)\n",
    "\n",
    "cv2.imwrite(r\"C:\\Users\\star\\Desktop\\b.jpg\", img_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for item in \"Yu\".split():\n",
    "    print('\\n'.join([''.join([(item[(x-y) % len(item)] if ((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3 <= 0 else ' ') for x in range(-30, 30)]) for y in range(12, -12, -1)]))\n",
    "    time.sleep(1.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 09:44:08.383 | ERROR    | __main__:<module>:30 - webdriver error occurred \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b20d408ed007>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         captcha = wait.until(\n\u001b[0;32m     23\u001b[0m             EC.presence_of_element_located(\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.geetest_slicebg.geetest_absolute'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             )\n\u001b[0;32m     26\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\star\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py\u001b[0m in \u001b[0;36muntil\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mscreen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'screen'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mstacktrace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stacktrace'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import time\n",
    "from loguru import logger\n",
    "\n",
    "COUNT = 1000\n",
    "\n",
    "for i in range(1, COUNT + 1):\n",
    "    try:\n",
    "        browser = webdriver.Chrome()\n",
    "        wait = WebDriverWait(browser, 10)\n",
    "        browser.get('https://captcha1.scrape.center/')\n",
    "        button = wait.until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (By.CSS_SELECTOR, '.el-button')\n",
    "            )\n",
    "        )\n",
    "        button.click()\n",
    "        captcha = wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.CSS_SELECTOR, '.geetest_slicebg.geetest_absolute')\n",
    "            )\n",
    "        )\n",
    "        time.sleep(5)\n",
    "        captcha.screenshot(f'data/captcha/images/captcha_{i}.png')\n",
    "    except WebDriverException as e:\n",
    "        logger.error(f'webdriver error occurred {e.msg}')\n",
    "    finally:\n",
    "        browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export https_proxy=http://127.0.0.1:33210 http_proxy=http://127.0.0.1:33210 all_proxy=socks5://127.0.0.1:33211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Python-urllib/3.6\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-63f2d570-600b36e754bbf93f64b3b6c8\"\n",
      "  }, \n",
      "  \"origin\": \"103.135.102.146\", \n",
      "  \"url\": \"https://httpbin.org/get\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# urllib set proxy\n",
    "# http/https\n",
    "\n",
    "from urllib.error import URLError\n",
    "from urllib.request import ProxyHandler, build_opener\n",
    "\n",
    "\n",
    "# 使用 ProxyHander 设置代理，参数是字典类型，键名为协议类型，键值是代理\n",
    "# 当请求为 http 协议的时候，会使用 http 键名对应的代理，当请求连接为 https 协议的时候，会使用 https 键名对应的代理\n",
    "proxy = \"127.0.0.1:33210\"\n",
    "proxy_handler = ProxyHandler({\n",
    "    \"http\": \"http://\" + proxy,\n",
    "    \"https\": \"https://\" + proxy\n",
    "})\n",
    "# 创建完 ProxyHander 对象之后，需要利用 build_opener 方法传入该对象来创建一个Opener\n",
    "# 这样就相当于此 Opener 已经设置好代理了，接下来直接调用 Opener 对象的 open 方法，即可访问我们所想要的链接\n",
    "opener = build_opener(proxy_handler)\n",
    "\n",
    "try:\n",
    "    response = opener.open(\"https://httpbin.org/get\")\n",
    "    print(response.read().decode(\"utf-8\"))\n",
    "except URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Python-urllib/3.6\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-63f2d6ef-32ffc96229b50a384c902e4e\"\n",
      "  }, \n",
      "  \"origin\": \"103.135.102.146\", \n",
      "  \"url\": \"https://httpbin.org/get\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# socks5 代理\n",
    "\n",
    "import socks\n",
    "import socket\n",
    "from urllib import request\n",
    "from urllib.error import URLError\n",
    "\n",
    "socks.set_default_proxy(socks.SOCKS5, '127.0.0.1', 33211)\n",
    "socket.socket = socks.socksocket\n",
    "try:\n",
    "    response = request.urlopen('https://httpbin.org/get')\n",
    "    print(response.read().decode('utf-8'))\n",
    "except URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.27.1\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-63f2d817-6b0db1d919d2d59f35c0f620\"\n",
      "  }, \n",
      "  \"origin\": \"103.135.102.146\", \n",
      "  \"url\": \"https://httpbin.org/get\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# requests set proxy\n",
    "\n",
    "import requests\n",
    "\n",
    "# If account password authentication is required, \"username:password@127.0.0.1:33210\"\n",
    "proxy = \"127.0.0.1:33210\"\n",
    "proxies = {\n",
    "    \"http\": \"http://\" + proxy,\n",
    "    \"https\": \"http://\" + proxy\n",
    "}\n",
    "try:\n",
    "    response = requests.get(\"https://httpbin.org/get\", proxies=proxies)\n",
    "    print(response.text)\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(\"Error\", e.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.27.1\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-63f2d938-0a1caf585bf4fe1d347d9409\"\n",
      "  }, \n",
      "  \"origin\": \"103.135.102.146\", \n",
      "  \"url\": \"https://httpbin.org/get\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# requests socks5\n",
    "\n",
    "import requests\n",
    "\n",
    "proxy = \"127.0.0.1:33211\"\n",
    "proxies = {\n",
    "    \"http\": \"socks5://\" + proxy,\n",
    "    \"https\": \"socks5://\" + proxy\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"https://httpbin.org/get\", proxies=proxies)\n",
    "    print(response.text)\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(\"Error: \", e.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import socks\n",
    "import socket\n",
    "\n",
    "socks.set_default_proxy(socks.SOCKSS, \"127.0.0.1\", \"33211\")\n",
    "socket.socket = socks.socksocket\n",
    "try:\n",
    "    response = requests.get(\"https://httpbin.org/get\")\n",
    "    print(response.text)\n",
    "except request.exceptions.ConnectionError as e:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-httpx/0.22.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-63f2e06a-32d5956e72fb156a2b80ca0d\"\n",
      "  }, \n",
      "  \"origin\": \"103.135.102.146\", \n",
      "  \"url\": \"http://httpbin.org/get\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# httpx proxy\n",
    "\n",
    "import httpx\n",
    "\n",
    "proxy = \"127.0.0.1:33210\"\n",
    "proxies = {\n",
    "    \"http://\": \"http://\" + proxy,\n",
    "    \"https://\": \"http://\" + proxy\n",
    "}\n",
    "\n",
    "with httpx.Client(proxies=proxies) as client:\n",
    "    response = client.get(\"http://httpbin.org/get\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-httpx/0.22.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-63f2e126-34574a8b029a14513455cc2b\"\n",
      "  }, \n",
      "  \"origin\": \"103.135.102.146\", \n",
      "  \"url\": \"http://httpbin.org/get\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "from httpx_socks import SyncProxyTransport\n",
    "\n",
    "trasport = SyncProxyTransport.from_url(\n",
    "    \"socks5://127.0.0.1:33211\"\n",
    ")\n",
    "\n",
    "with httpx.Client(transport=trasport) as client:\n",
    "    response = client.get(\"http://httpbin.org/get\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium proxy\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "proxy = \"127.0.0.1:33211\"\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--proxy-server=http://' + proxy)\n",
    "browser = webdriver.Chrome(options=options)\n",
    "browser.get('https://httpbin.org/get')\n",
    "print(browser.page_source)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiohttp proxy\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "proxy = \"http://127.0.0.1:33211\"\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(\"https://httpbin.org/get\", proxy=proxy) as response:\n",
    "            print(await response.text())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.get_event_loop().run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiohttp socks\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from aiohttp_socks import ProxyConnector\n",
    "\n",
    "connector = ProxyConnector.from_url(\"socks5://127.0.0.1:33211\")\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        async with session.get(\"https://httpbin.org/get\") as response:\n",
    "            print(await response.text())\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.get_event_loop().run_until_complete(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f8831d21f27ca7ea35646f39de7dd31bf318bfdb0c4c19dd35ff000f9b26728"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
